description: "ELM ITV Benchmarks (Efficient Language Models - Is It Viable?)"
providers: 

  # Control CLOUD LLMs
  - id: openai:gpt-3.5-turbo-1106

  # Experimental Local ELMs
  - id: ollama:chat:llama3
    config:
      modelName: "llama3"
      stream: false
      temperature: 0.2
  - id: ollama:chat:phi3
    config:
      modelName: "phi3"
      stream: false
      temperature: 0.2
  - id: ollama:chat:gemma
    config:
      modelName: "gemma"
      stream: false
      temperature: 0.2
  
  # IF YOUR COMPUTER CAN HANDLE IT
  # - id: ollama:chat:llama3:70b
  #   config:
  #     modelName: "llama3:70b"
  #     stream: false
  #     temperature: 0.2

  # --------------------------------------------------
  
  # Experimental Local ELMs (with tokens information)
  # Use this to get the token usage information
  # - id: ../custom_models/ollamaModelBase.js
  #   config:
  #     modelName: "llama3"
  #     stream: false
  #     temperature: 0.2
  # - id: ../custom_models/ollamaModelBase.js
  #   config:
  #     modelName: "phi3"
  #     stream: false
  #     temperature: 0.2
  # - id: ../custom_models/ollamaModelBase.js
  #   config:
  #     modelName: "gemma"
  #     stream: false
  #     temperature: 0.2
  # IF YOUR COMPUTER CAN HANDLE IT 
  # - id: ../custom_models/ollamaModelBase.js
  #   config:
  #     modelName: "llama3:70b"
  #     stream: false
  #     temperature: 0.2
evaluateOptions:
  repeat: 1
